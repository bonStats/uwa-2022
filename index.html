<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Twisted: Improving particle filters</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua J Bon" />
    <script src="libs/header-attrs-2.13/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="bon-qut-campus-title.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Twisted: Improving particle filters
## by learning modified paths
### Joshua J Bon
### Mathematics Sciences, QUT
### 18-05-2022

---

class: list-space


&lt;style&gt;

.list-space li {
padding: 0.25cm;
}

.list-nobullet li {
  list-style-type:none;
}

&lt;/style&gt;

## Acknowledgements 

Joint work with 

- **Anthony Lee** (Bristol University) 
- **Christopher Drovandi** (QUT)


Institutions and Groups

- **School of Mathematical Sciences, QUT**
- **QUT Centre for Data Science**

---
class: list-space

## My research area

.full-width[.content-box-red[
**Adaptive methods** for sequential Monte Carlo **tuned with particles**
]]

--

This work...

.full-width[.content-box-blue[
Particle filters for dynamic models with **difficult/intractable transition densities**
]]

---

## Hidden Markov models

.pull-left[.content-box-purple[

- Inference over latent states
 - Time series
 - Robotics

- Unbiased estimate of normalising constant
 - Model selection
 - Pseudo-marginal methods

]]

.pull-right[

&lt;img src="imgs/hmm_tikz.svg" width="600" style="display: block; margin: auto;" /&gt;

]

--

`$$\hat{Z}_n = \int_{\mathsf{X}^{n+1}}m_0(x_0)G_0(x_0,y_0)\prod_{p=1}^{n}m_p(x_{p-1},x_{p})G_p(x_p,y_p)\text{d}x_0\cdots\text{d}x_n$$`

- Mutation: `\(m_p\)` is the density for `\(M_p\)`, potential: `\(G_p\)` is the likelihood of `\(y_p\)`

- Particle filters exploit the conditional structure

---

## üåà Everything is an integral

--

If `\(\mu\)` is a probability distribution (or measure) on measurable space `\((\mathsf{X},\mathcal{X})\)`,

for `\(S \in \mathcal{X}\)` and measurable `\(\varphi:\mathsf{X}\rightarrow \mathbb{R}\)`

.content-box-purple[

`$$\mu(S) = \int_{S} \mu(\text{d}x)$$`
`$$\quad \mu(\varphi) = \int_{\mathsf{X}} \varphi(x) \mu(\text{d}x)$$`
]


--
&lt;br&gt;

- Distributions (or measures) defined implicitly by `\(\varphi(x) \mu(\text{d}x) \Longleftrightarrow \int_{S} \varphi(x) \mu(\text{d}x)\)`

---

## üåà Everything is an integral

If `\(\mu\)` is a probability distribution (or measure) on measurable space `\((\mathsf{X},\mathcal{X}) = \color{red}{(\mathbb{R},\mathscr{B}(\mathbb{R}))}\)`,

for `\(S \in \mathcal{X}\)` and measurable `\(\varphi:\mathsf{X}\rightarrow \mathbb{R}\)`

.content-box-purple[

`$$\mu(S) = \int_{S} \color{red}{p_\mu(x)\text{d}x}$$`

`$$\mu(\varphi) = \int_{\mathsf{X}} \varphi(x) \color{red}{p_\mu(x)\text{d}x}$$`
]


&lt;br&gt;
- Distributions (or measures) defined implicitly by `\(\varphi(x) \mu(\text{d}x) \Longleftrightarrow \int_{S} \varphi(x) \color{red}{p_\mu(x)\text{d}x}\)`


---
class: list-space

## üåà Everything is an integral

If `\(K\)` is a Markov kernel (or non-negative kernel) on measure space `\((\mathsf{X},\mathcal{X})\)`,

for `\(v \in \mathsf{X}\)`, `\(S \in \mathcal{X}\)`, and measurable `\(\varphi:\mathsf{X}\rightarrow \mathbb{R}\)`

.content-box-purple[

`$$K(v,S) = \int_{S} K(v,\text{d}x)$$`

`$$K(\varphi)(v) = \int_{\mathsf{X}} \varphi(x) K(v,\text{d}x)$$`
]

--

- For a fixed `\(v \in \mathsf{X}\)`, `\(K(v,\cdot)\)` is a measure
- `\(K(\varphi)(\cdot)\)` is a measurable function, allows `\(\mu(K(\varphi))\)` notation

---
class: inverse, center, middle, hide-logo

# From Feynman-Kac models to particle filters

---
class: list-space

## 1-step Feynman-Kac model

.pull-left[
![](index_files/figure-html/fk-viz-1-1.png)&lt;!-- --&gt;
]
.pull-right[
.content-box-purple[
`$$\eta_0 \overset{G_0}{\longrightarrow} \hat\eta_0 \overset{M_1}{\longrightarrow} \eta_1$$`
]

]

---
class: list-space

## 1-step particle filter

.pull-left[
![](index_files/figure-html/fk-viz-2-1-1.png)&lt;!-- --&gt;
]
.pull-right[

.content-box-purple[
`$$\eta_0^N \overset{G_0}{\longrightarrow} \hat\eta_0^N \overset{M_1}{\longrightarrow} \eta_1^N$$`
]


]

---

## n-step model

With `\(\gamma_0 = M_0\)` we can define marginal *predictive* **measures** and **distributions** via recursion 

.content-box-purple[
`$$\gamma_{p}(S) = \int_{\mathsf{X}} \gamma_{p-1}(\text{d}x_{p-1})\color{red}{G_{p-1}(x_{p-1})}\color{blue}{M_{p}(x_{p-1},S)} \qquad\qquad
\eta_{p}(S) = \frac{\gamma_{p}(S)}{Z_p}$$`
]

--

And *updated* counterparts

.content-box-purple[
`$$\hat{\gamma}_{p}(\text{d}x_p) = \gamma_{p}(\text{d}x_p)\color{red}{G_p(x_p)} \qquad\qquad
\hat{\eta}_{p}(S) = \frac{\hat{\gamma}_{p}(S)}{\hat{Z}_p}$$`
]

---

## Feynman-Kac summary

### Marginal space

`\(x_{p} \in \mathsf{X}\)` for `\(p \in \{0,1,\ldots,n\}\)`

|             | Measure             | Distribution        |
| ----------- | :-----------------: | :-----------------: |
| Predictive  | `\(\gamma_p\)`          | `\(\eta_p\)`            |
| Updated     | `\(\hat{\gamma}_p\)`    | `\(\hat{\eta}_p\)`      |
| Normalised  | ‚ùå| ‚úÖ |




---
class: list-space

## Feynman-Kac summary

- Feynman-Kac model defines
 - Path measure (and distribution)
 - Sequence of marginal distributions 
 - How we traverse these

--
- Infinitely many models to perform the same statistical inference

--
- Many **bad** models for SMC possible (variance of estimates)

--

&lt;br&gt;
.content-box-purple[

How do we improve an existing Feynman-Kac model (for use in SMC)?

]

---
class: inverse, center, middle, hide-logo

## Twisted Feynman-Kac models

---
class: list-space

## Twisting or change of measure

.content-box-purple[
`$$\mu^\psi(\text{d} x) = \frac{\mu(\text{d} x) \psi(x)}{Z}$$`
]

with `\(Z = \mu(\psi) = \int_{\mathsf{X}}\psi(x) \mu(\text{d} x) &lt; \infty\)`.

--
- Exponential tilting/twisting: `\(\psi(x) = \exp(-\lambda x)\)`

--
- If continuous, new density `\(p_{\mu^{\psi}}(x) \propto p_\mu(x)\psi(x)\)`

---
class: list-space

## 1-step Twisted Feynman-Kac

.pull-left[
![](index_files/figure-html/fk-viz-1-tw-1.png)&lt;!-- --&gt;
]
.pull-right[
.content-box-purple[
`$$\eta_0^\psi \overset{G_0^\psi}{\longrightarrow} \hat\eta_0^\psi \overset{M_1^\psi}{\longrightarrow} \eta_1^\psi$$`
]
- `\(\eta_1^\psi = \eta_1\)`, twisting preserves path/terminal **distribution**
- Using optimal `\(\psi\)`, **potentials** are constant

]

---
class: list-space

## 1-step TFK particle filter

.pull-left[
![](index_files/figure-html/fk-viz-2-tw-1.png)&lt;!-- --&gt;
]
.pull-right[
.content-box-purple[
`$$(\eta_0^\psi)^N \overset{G_0^\psi}{\longrightarrow} (\hat\eta_0^\psi)^N \overset{M_1^\psi}{\longrightarrow} (\eta_1^\psi)^N$$`
]
- Using optimal `\(\psi\)`, **weights** are constant
- And no resampling required
]

---

## Twisting Feynman-Kac models

Twist the Markov kernels with any measurable `\(\psi = \{\psi_0, \ldots, \psi_n\}\)`

.content-box-purple[
`$$\begin{aligned}M^{\color{red}{\psi}}_0(\text{d}x_0) &amp;= \frac{M_0(\text{d}x_0)\color{red}{\psi_0}(x_0)}{M_0(\color{red}{\psi_0})} \\
&amp;~\\&amp;~\\
M^{\color{red}{\psi}}_p(x_{p-1}, \text{d}x_p) &amp;= \frac{M_p(x_{p-1}, \text{d}x_p)\color{red}{\psi_p}(x_p)}{M_p(\color{red}{\psi_p})(x_{p-1})}\end{aligned}$$`
]

---

## Twisting Feynman-Kac models

Also "twist" the potential functions 

.content-box-purple[
`$$\begin{aligned} G^{\color{red}{\psi}}_0(x_0) &amp;= \frac{G_0(x_0)}{\color{red}{\psi_0}(x_0)}M_1(\color{red}{\psi_1})(x_0)M_0(\color{red}{\psi_0})\\
&amp;~\\
G^{\color{red}{\psi}}_p(x_p) &amp;= \frac{G_p(x_p)}{\color{red}{\psi_p}(x_p)}M_{p+1}(\color{red}{\psi_{p+1}})(x_p)\\
&amp;~\\
G^{\color{red}{\psi}}_n(x_n) &amp;= \frac{G_n(x_n)}{\color{red}{\psi_n}(x_n)}\end{aligned}$$`
]

--

&lt;br&gt;&lt;br&gt;
Defines new Feynman-Kac model, can be used by standard (e.g. bootstrap) particle filter

---

## Twisting Feynman-Kac models

Why??

`$$\hat{\boldsymbol{\gamma}}^{\psi}_{n}(\text{d}x_{0:n}) = M_0^\psi(\text{d}x_0)G^{\psi}_0(x_0)\prod_{p=1}^{n}M_p(x_{p-1},\text{d}x_p)G^{\psi}_p(x_p)$$`

--


.content-box-purple[
Preserves terminal quantities
`$$\hat\eta^{\psi}_{n} = \hat\eta_{n}, \qquad\hat{\gamma}^{\psi}_{n} = \hat{\gamma}_{n}, \qquad \hat{Z}^{\psi}_n = \hat{Z}_n$$`
]

--

But does change intermediate distributions, `\(\eta^{\psi}_p \neq \eta_p\)`

---

## Optimal twisting

If we use twisting functions that incorporate all future information

.content-box-red[
`$$\psi_p^{\star}(x_p) = \mathbb{E}\left(\prod_{t=p}^{n}G_t(X_t)~\Big\vert~ X_p = x_p\right)$$`
]

with `\(X_{0:n} \sim \boldsymbol{P}_n\)` (the underlying Markov chain), then 

--

.content-box-purple[
`$$(\hat{Z}^{\psi})^{N} \overset{a.s.}{=}  \hat{Z}$$` 
]

--

and **exact** samples from particle filter with finite `\(N\)` (!) for the terminal distribution

---

## Iterated APF and Controlled SMC

- IAPF [(Guarniero et al, 2017)](https://doi.org/10.1080/01621459.2016.1222291) and CSMC [(Heng et al, 2020)](https://doi.org/10.1214/19-AOS1914) use twisted FK models
- Learn twisting functions using a recursion 

--

The optimal twisting functions satisfy

`$$\psi_n^{\star}(x_n) = G_n(x_n)$$`

`$$\psi_p^{\star}(x_p) = G_p(x_p)M_{p+1}(\psi_{p+1}^{\star})(x_p)$$`

--

Motivates recursive, iterative learning...

--

.content-box-purple[
**Algorithm:** For class of approximate twisting function `\(\tilde{\psi}_p \in \mathsf{H}\)`

1. Run particle filter with current `\(\psi\)`, generate `\(\zeta_p^i\)`
2. Find new approximation of `\(\tilde{\psi}_{p}(\zeta_p^i) \approx G_p(\zeta_p^i)M_{p+1}(\tilde{\psi}_{p+1})(\zeta_p^i)\)` with backward recursion
3. Repeat
]

---

## Existing applications of recursive learning

Twisted mutation `\(M_p^{\psi}\)` can be sampled from and constant `\(M_p(\psi_p)\)` can be calculated **analytically**.

--

.content-box-purple[
Normal models and exponential-quadratic `\(\psi\)`
`$$M_p = \mathcal{N}(f(x_{p-1}),\Sigma)$$`
`$$\psi_p(x_p) = \exp\left(- x_{p}^{\top} A x_{p} + bx_{p} + c \right)$$`
where `\(A\)` is PSD.

Then `\(M_p^{\psi}(x_{p-1}, \cdot) = \mathcal{N}(f_{\psi}(x_{p-1}),\Sigma_{\psi})\)` can be calculated **analytically**
]


--
‚ùó Almost all current methods are for Gaussian mutation kernels and twisting functions

---
class: inverse, center, middle, hide-logo

## Beyond exact twisting ‚≠ê

---

## Extending beyond the analytical case 

What if there is no class of appropriate `\(\psi\)` functions that are tractable?

- `\(M_p\)` intractable or difficult to work with
- `\(M_p^{\psi}\)` unable to be sampled from directly

--
&lt;br&gt;&lt;br&gt;&lt;br&gt;
.content-box-blue[
**Idea**: Rejection sampler + Unbiased estimate of twisted potentials
]

---
class: list-space

## Extending beyond the analytical case 

### Rejection sampler

- Restrict `\(\psi: \mathsf{X}\rightarrow [0,1]\)`
- Propose `\(X \sim M_p\)` until `\(\psi_p(X) &gt; U\)`, where `\(U \sim \text{Uniform}(0,1)\)`

--

Accepted realisations have the correct twisted distribution: `\(X \sim M_p^{\psi}\)`

---
## Extending beyond the analytical case 

### Unbiased estimate of twisted potentials

`$$\tilde{G}_p^{\psi}(x_p) = \frac{G_p(x_p)}{\psi_p(x_p)}\color{green}{\tilde{M}_{p+1}(\psi_{p+1})(x_p)}$$`

--

.content-box-purple[

`$$\tilde{M}_{p+1}(\psi_{p+1})(x_p) = K^{-1}\sum_{i=1}^{K}\psi_{p+1}(U^{i}_{p+1}) \qquad \qquad \qquad U^{i}_{p+1} \sim M_{p+1}(x_p, \cdot)$$`

]

---
class:list-space

## Twisted models by rejection-sampling and unbiased potentials

Applicable to **any** Feynman-Kac model with bounded `\(\psi\)` and where mutations can be sampled from, using 

--

.pull-left[.content-box-purple[
**Rejection sampler** for twisted mutation kernel

- Exact twisting
- Potentially costly sampler
]]


--
.pull-right[.content-box-purple[
**MC estimate** for twisted potential function

- Simple 
- Potentially noisy estimate
]]


--

&lt;br&gt;&lt;br&gt;&lt;br&gt;
How to address concerns?

---
class: list-space

## Controlling cost of rejection sampler

Rejection sampler for `\(M^{\omega}_p\)` has acceptance rate: 

`$$M_p(\omega_p)(x_{p-1}) = \int_{\mathsf{X}}\omega_p(x_p)M_p(x_{p-1}, \text{d}x_p)$$`
Normalising constant conditional on `\(x_{p-1}\)`.


--
Average acceptance rate (rejection sampler within SMC sampler) is

.content-box-purple[
`$$\begin{aligned}
\alpha_p^{\omega} &amp;= \hat{\eta}_{p-1}^{\omega}(M_p(\omega_p)) ~\text{for}~p=1,\ldots,n\\ 
\alpha_0 &amp;= M_0(\omega_{0})\end{aligned}$$`
]

--

How to estimate before running the particle filter (for `\(p\neq 0\)`)?

---

## Controlling cost of rejection sampler

.content-box-red[
**Prop.** If we have a `\(\psi\)`-twisted Feynman-Kac model, then the average acceptance rates of a `\(\omega\)`-twisted model can be written as

`$$\begin{aligned}
\alpha_{p}^{\omega} &amp;=  \frac{\hat\eta_{p-1}^{\psi}( M_{p}(\omega_{p})^2 \cdot M_{p}(\psi_{p})^{-1} )}{\hat\eta_{p-1}^{\psi}( M_{p}(\omega_{p}) \cdot M_{p}(\psi_{p})^{-1})} ~\text{for}~p \in [n]
\end{aligned}$$`
]


--
‚ùó A quantity for average RS acceptance rates within a particle filter


--
‚ÄºÔ∏è Estimate without running the particle filter for the `\(\omega\)`-twisted FK model


--
.content-box-purple[
**Target `\(\alpha\)`**: Use in the iterative learning algorithm and temper the new twisting functions
]


---
class: list-space

## Sketch of proof

üëâ New twisting functions decomposed as `\(\omega_p = \psi_p \cdot \phi_p\)`

Two nice properties

- `\((\hat\gamma_{p-1}^{\psi})^\phi = \hat\gamma_{p-1}^{\psi\cdot\phi}\)`
- `\(\hat\gamma_{p-1}^\phi(\text{d} x_{p-1}) = \hat{\gamma}_{p-1}(\text{d} x_{p-1}) M_{p}(\phi_{p})(x_{p-1})\)`


--
Yields

- `\(\hat{\gamma}_{p-1}^{\omega}(\text{d} x_{p-1}) = \hat{\gamma}_{p-1}^{\psi}(\text{d} x_{p-1}) M_{p}^{\psi}(\phi_{p})(x_{p-1})\)`


--
Use to express `\(\hat{\eta}_{p-1}^{\omega}(M_p(\omega_p))\)` without `\(\omega\)`-twisting

And substitute `\(M_{p}^{\psi}(\phi_{p}) = \frac{ M_{p}(\omega_{p})}{M_{p}(\psi_{p})}\)` everywhere


---

## Analysing PF variance with estimate of twisted potentials

.content-box-red[
**Prop**: If the relative variance of random-potential is uniformly bounded, that is
$$
\mathrm{Var}\left(\frac{\tilde{G}_p(\tilde{x}_p)}{G_p(x_p)}~\Big\vert~ x_p\right) &lt; C
$$
then the asymptotic variance satisfies

`$$\tilde{\sigma}_n^{2}(\varphi \otimes 1) &lt; (C+1) \sigma_n^{2}(\varphi)$$`
]

---
class: list-space

## Rough sketch of proof

- Define a new equivalent Feynman-Kac model on augmented state-space `\(X_p, U_p\)`
    - `\(U_p \sim M_{p+1}(x_p, \cdot)\)`
- Asymptotic variance

`$$\begin{aligned}
\tilde{\sigma}^2_n(\varphi \otimes 1) &amp;= N^{-1}\sum_{p=0}^{n} \tilde{v}_{p,n}(\varphi \otimes 1)
\end{aligned}$$`

Bound each `\(\tilde{v}_{p,n}(\varphi)\)` using  `\(\mathrm{Var}\left(\frac{\tilde{G}_p(\tilde{x}_p)}{G_p(x_p)}~\Big\vert~ x_p\right) &lt; C\)`

--

&lt;br&gt;&lt;br&gt;
‚ùó Effect of using estimate is isolated, does not compound (asymptotically)


---

## Example: Linear Gaussian Hidden Markov model

Model

- `\(d=3\)`, `\(n = 200\)`
- `\(a_0 = [1~1~1]\)`, `\(\Sigma_0 = I_{d}\)`
- `\(\Sigma_M = I_{d}\)`
- `\(\Sigma_G = \sigma^2_{G}I_{d}\)` with `\(\sigma^2_{G} \in \{0.25,1.0\}\)`
- `\(A\)` are such that `\(A_{i,j} = a^{\vert i-j\vert + 1}\)` with `\(a = 0.42\)`

Algorithm

- Exponential quadratic twisting function
- 3 iterations of iterative learning
    - `\(\alpha\)` target `\(= 0.04, 0.02, 0.01\)`
- `\(N = 200\)` particles for twisted models
- Dynamic multinomial resampling with `\(N/2\)` threshold
- `\(\tilde{N} \in \{25,50,100\}\)`
 
Tested over 100 repetitions

- compare against memory (S = particle storage)
- computation (C = number of mutation draws)

---


&lt;img src="imgs/log-sim-Z-lghmm-res.svg" width="640" style="display: block; margin: auto;" /&gt;

---
class: list-space

## Partial exact twisting

.content-box-red[
With tempering, controlling acceptance rate alters twisting functions from those learned. 
]


--
For some Markov kernels

- Decompose twisting functions `\(\psi_p = \varrho_p \cdot \omega_p\)` such that
- An exact sampler for `\(M_p^\varrho\)` exists
- Then use rejection sampling for remainder `\(\omega_p\)`


--
.content-box-blue[
**Idea**: Choose `\(\varrho_p\)` to maximise acceptance rates
]

---

## Stochastic Volatility Model

.content-box-purple[
`\(R_t\)` returns with variance `\(X_t\)` governed by
`$$\begin{split}
    R_t &amp;= X_t^{1/2}Z_t \\
    d X_t &amp;= (\phi_1 - \phi_2 X_{t}) d t +\phi_3 X_t^{1/2} dW_t
\end{split}$$`
]

- `\(Z_t \sim \mathcal{N}(0,\sigma^2)\)`
- Brownian motion `\(W_t\)`
- Model from e.g. [Martin et al, 2019](https://doi.org/10.1080/10618600.2018.1552154)

--


- Unit increments of `\(X_t\)` follow scaled non-central chi-square distribution
- Can be exponentially tilted


---
## Example: Stochastic Volatility Model

Data

- `\(n = 2000\)`
- `\(\phi_1 =0.1\)`, `\(\phi_2 = 0.5\)`, `\(\phi_3 = 0.1\)`, and `\(\sigma = 0.25\)` 

Model

- `\(\phi_1 =0.09\)`, `\(\phi_2 = 0.45\)`, `\(\phi_3 = 0.11\)`, and `\(\sigma = 0.25\)`

Algorithm

- Exp-quadratic twisting (rejection) + Exp-linear twisting (analytical)
- 2 iterations of iterative learning, `\(N = 100\)`
- Final twisted particle filter  `\(N = 250\)`
- Dynamic multinomial resampling with `\(N/2\)` threshold
 
Tested over 100 repetitions

- compare against memory (S = particle storage)
- computation (C = number of mutation draws)

---

&lt;img src="imgs/sv-log-sim-Z-N100-100-250-MC2-NL-10-sig025.png" width="700" style="display: block; margin: auto;" /&gt;


---
class: list-space

## Conclusions

- Twisting models can define perfect samplers

--
- Analytical twisting limited to only few models (i.e. Bayesian conjugacy)

--
- Extend with rejection sampling, high 'naive' computational cost but memory usage reduced

--
- Partial-rejection options may be available

--
- Future: new models, identify characteristics of models that will benefit (computational aspect)

---
class: inverse, center, middle, hide-logo

## Thank you for listening!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(imgs/QUT_SQUARE_RGB_XLGE.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 100px;
  height: 100px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
